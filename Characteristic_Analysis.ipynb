{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8851f479-7eee-4505-94e2-c3c35e0c8be6",
   "metadata": {},
   "source": [
    "# Characteristic Analysis for LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf704ce-4c49-4ae2-8481-cf4db5a0efde",
   "metadata": {},
   "source": [
    "## Five Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "178fdd26-5f1d-48eb-89c5-19f90d17272e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counts for each LLM model:\n",
      "Claude-3-haiku:\n",
      "  Train: 3102\n",
      "  Val: 775\n",
      "  Test:  969\n",
      "\n",
      "GPT-4o-mini:\n",
      "  Train: 3047\n",
      "  Val: 762\n",
      "  Test:  952\n",
      "\n",
      "Llama-3.2-3B-Instruct:\n",
      "  Train: 3174\n",
      "  Val: 794\n",
      "  Test:  993\n",
      "\n",
      "Phi-3-Mini-4K:\n",
      "  Train: 3198\n",
      "  Val: 800\n",
      "  Test:  1000\n",
      "\n",
      "Qwen2.5-3B-Instruct:\n",
      "  Train: 2979\n",
      "  Val: 745\n",
      "  Test:  931\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/CSE584_Midterm/Model_Evaluation/load_and_preprocess.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_df['LLM_encoded'] = le.fit_transform(combined_df['LLM'])\n"
     ]
    }
   ],
   "source": [
    "from Model_Evaluation.load_and_preprocess import load_and_preprocess_data, preprocess_data, create_data_loaders, TextDataset\n",
    "import numpy as np\n",
    "import random \n",
    "import pandas as pd\n",
    "\n",
    "# SEED FIXED\n",
    "SEED = 20241006\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "file_paths = ['DataSet/X_ij_samples_5k_Claude-3-haiku_5sentences.csv', \n",
    "              # 'DataSet/X_ij_samples_5k_Falcon-7b.csv', \n",
    "              'DataSet/X_ij_samples_5k_Qwen2.5-3B-Instruct_5sentences.csv',\n",
    "              'DataSet/X_ij_samples_5k_GPT-4o-mini_5sentences.csv',\n",
    "              'DataSet/X_ij_samples_5k_Llama-3.2-3B-Instruct_5sentences.csv',\n",
    "              # 'DataSet/X_ij_samples_5k_Llama-2-7b-chat.csv',\n",
    "              'DataSet/X_ij_samples_5k_Phi-3-Mini-4K_5sentences.csv']\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val, label_encoder, combined_df, combined_df_temp = load_and_preprocess_data(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff90a28-c72f-47a2-9307-9aa797ffd60f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      processed_text                    LLM\n",
      "0  but they all looked bland on my kitchen counte...  Llama-3.2-3B-Instruct\n",
      "1  my world before you came and turned everything...  Llama-3.2-3B-Instruct\n",
      "2  after two hours , she was still working on her...    Qwen2.5-3B-Instruct\n",
      "3  the rest of the dress hugged my figure beautif...    Qwen2.5-3B-Instruct\n",
      "4  her eyes whipped around the room, searching fo...         Claude-3-haiku\n"
     ]
    }
   ],
   "source": [
    "def restore_test_dataframe(X_test, y_test, label_encoder):\n",
    "    \n",
    "    y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "    \n",
    "    restored_df = pd.DataFrame({\n",
    "        'processed_text': X_test,  \n",
    "        'LLM': y_test_labels       \n",
    "    })\n",
    "    \n",
    "    return restored_df\n",
    "\n",
    "restored_test_df = restore_test_dataframe(X_test, y_test, label_encoder)\n",
    "\n",
    "print(restored_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1caedf4e-a9c2-4391-87ae-56ce1a51cd70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = combined_df.reset_index(drop=True)\n",
    "# df = restored_test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1208d6-e615-4cb6-be4a-8f6abf8d458b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### N-gram Frequency Analysis (Bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2df9ac6-0f27-45cf-802f-8a5aebdbe944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Bigrams for Claude-3-haiku:\n",
      "deep breath       199\n",
      "felt sense        195\n",
      "moving forward    131\n",
      "hard work         127\n",
      "couldn help       126\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for Qwen2.5-3B-Instruct:\n",
      "felt like              288\n",
      "yesterday evening      286\n",
      "yesterday morning      184\n",
      "yesterday afternoon    112\n",
      "feels like              92\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for GPT-4o-mini:\n",
      "deep breath    292\n",
      "took deep      176\n",
      "felt sense     172\n",
      "filled air     127\n",
      "hard work      123\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for Llama-3.2-3B-Instruct:\n",
      "couldn help    225\n",
      "felt like      116\n",
      "living room     94\n",
      "best friend     85\n",
      "coffee shop     74\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for Phi-3-Mini-4K:\n",
      "felt sense       131\n",
      "living room      119\n",
      "couldn help      115\n",
      "end day          109\n",
      "floor cleaner     93\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')  \n",
    "X = vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "ngram_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "ngram_freq['LLM'] = df['LLM']\n",
    "\n",
    "for llm in df['LLM'].unique():\n",
    "    print(f\"\\nTop Bigrams for {llm}:\")\n",
    "    llm_ngrams = ngram_freq[ngram_freq['LLM'] == llm].drop('LLM', axis=1).sum().sort_values(ascending=False).head(5)\n",
    "    print(llm_ngrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bcab5e-97db-420e-9771-a0d7d1d10c62",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e125972b-499e-4365-8435-116eaa55b58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "['home', 'yesterday', 'friend', 'today', 'soon', 'just', 'work', 'quickly', 'time', 'like']\n",
      "\n",
      "Topic 2:\n",
      "['clear', 'people', 'challenges', 'sense', 'life', 'felt', 'experience', 'mind', 'despite', 'ultimately']\n",
      "\n",
      "Topic 3:\n",
      "['help', 'make', 'home', 'family', 'work', 'friends', 'decided', 'day', 'time', 'new']\n",
      "\n",
      "Topic 4:\n",
      "['park', 'away', 'laughter', 'day', 'air', 'felt', 'eyes', 'filled', 'moment', 'room']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=100)\n",
    "count_data = count_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=4, random_state=20241016)\n",
    "lda.fit(count_data)\n",
    "\n",
    "topic_distribution = lda.transform(count_data)\n",
    "df['Topic_1'] = topic_distribution[:, 0]\n",
    "df['Topic_2'] = topic_distribution[:, 1]\n",
    "df['Topic_3'] = topic_distribution[:, 2]\n",
    "df['Topic_4'] = topic_distribution[:, 3]\n",
    "# print(df[['LLM', 'Topic_1', 'Topic_2']])\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"\\nTopic {idx+1}:\")\n",
    "    print([count_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58e998a-ccdf-4ca6-add8-6e33dcf0a7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_topic_distribution:\n",
      "                     LLM   Topic_1   Topic_2   Topic_3   Topic_4\n",
      "0         Claude-3-haiku  0.163721  0.326478  0.245243  0.264559\n",
      "1            GPT-4o-mini  0.141053  0.291688  0.223890  0.343369\n",
      "2  Llama-3.2-3B-Instruct  0.273114  0.170729  0.296453  0.259704\n",
      "3          Phi-3-Mini-4K  0.185405  0.213478  0.306135  0.294982\n",
      "4    Qwen2.5-3B-Instruct  0.393773  0.132102  0.275452  0.198673\n"
     ]
    }
   ],
   "source": [
    "# LLM별 주제 분포 평균 계산\n",
    "average_topic_distribution = df.groupby('LLM')[['Topic_1', 'Topic_2', 'Topic_3', 'Topic_4']].mean().reset_index()\n",
    "\n",
    "print(\"average_topic_distribution:\")\n",
    "print(average_topic_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295ceaf-6482-4b92-a3d7-df69da0b225b",
   "metadata": {},
   "source": [
    "## Single Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "902e5452-fd26-4e26-80ab-f46b669cce38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counts for each LLM model:\n",
      "Claude-3-haiku:\n",
      "  Train: 3179\n",
      "  Val: 795\n",
      "  Test:  993\n",
      "\n",
      "Falcon-7b:\n",
      "  Train: 3168\n",
      "  Val: 792\n",
      "  Test:  990\n",
      "\n",
      "GPT-4o-mini:\n",
      "  Train: 3198\n",
      "  Val: 800\n",
      "  Test:  1000\n",
      "\n",
      "Llama-2-7b-chat:\n",
      "  Train: 3182\n",
      "  Val: 796\n",
      "  Test:  994\n",
      "\n",
      "Llama-3.2-3B-Instruct:\n",
      "  Train: 3198\n",
      "  Val: 799\n",
      "  Test:  1000\n",
      "\n",
      "Phi-3-Mini-4K:\n",
      "  Train: 3197\n",
      "  Val: 800\n",
      "  Test:  1000\n",
      "\n",
      "Qwen2.5-3B-Instruct:\n",
      "  Train: 3178\n",
      "  Val: 794\n",
      "  Test:  993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/CSE584_Midterm/Model_Evaluation/load_and_preprocess.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_df['LLM_encoded'] = le.fit_transform(combined_df['LLM'])\n"
     ]
    }
   ],
   "source": [
    "from Model_Evaluation.load_and_preprocess import load_and_preprocess_data, preprocess_data, create_data_loaders, TextDataset\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "# SEED FIXED\n",
    "SEED = 20241006\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "file_paths = ['DataSet/X_ij_samples_5k_Claude-3-haiku.csv', \n",
    "              'DataSet/X_ij_samples_5k_Falcon-7b.csv', \n",
    "              'DataSet/X_ij_samples_5k_Qwen2.5-3B-Instruct.csv',\n",
    "              'DataSet/X_ij_samples_5k_GPT-4o-mini.csv',\n",
    "              'DataSet/X_ij_samples_5k_Llama-3.2-3B-Instruct.csv',\n",
    "              'DataSet/X_ij_samples_5k_Llama-2-7b-chat.csv',\n",
    "              'DataSet/X_ij_samples_5k_Phi-3-Mini-4K.csv']\n",
    "\n",
    "X_train, X_test, X_val, y_train, y_test, y_val, label_encoder, combined_df_1sen, combined_df_temp = load_and_preprocess_data(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371cf9a-e8f9-49d4-bb3b-c1e82ea22199",
   "metadata": {},
   "source": [
    "### N-gram Frequency Analysis (Bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d30a78-4483-46f6-b7f7-e64efbea971e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Bigrams for Claude-3-haiku:\n",
      "late meeting        33\n",
      "surprise party      25\n",
      "deep breath         22\n",
      "beautiful sunset    21\n",
      "felt sense          18\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for Falcon-7b:\n",
      "walked away          30\n",
      "deep breath          27\n",
      "gravity situation    25\n",
      "sight behold         23\n",
      "realized gravity     18\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for Qwen2.5-3B-Instruct:\n",
      "yesterday evening    84\n",
      "yesterday morning    28\n",
      "dinner tonight       19\n",
      "work today           18\n",
      "late appointment     15\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for GPT-4o-mini:\n",
      "felt sense          30\n",
      "beautiful sunset    29\n",
      "finish homework     22\n",
      "deep breath         22\n",
      "finish project      20\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for Llama-3.2-3B-Instruct:\n",
      "high school       35\n",
      "best friend       35\n",
      "living room       34\n",
      "birthday party    32\n",
      "felt like         31\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for Llama-2-7b-chat:\n",
      "home work            23\n",
      "late work            21\n",
      "finished homework    19\n",
      "video games          19\n",
      "walked away          16\n",
      "dtype: int64\n",
      "\n",
      "Top Bigrams for Phi-3-Mini-4K:\n",
      "living room      25\n",
      "grocery store    22\n",
      "cup coffee       15\n",
      "walk park        15\n",
      "deep breath      14\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df2 = combined_df_1sen.reset_index(drop=True)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')  \n",
    "X = vectorizer.fit_transform(df2['processed_text'])\n",
    "ngram_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "ngram_freq['LLM'] = df2['LLM']\n",
    "\n",
    "for llm in df2['LLM'].unique():\n",
    "    print(f\"\\nTop Bigrams for {llm}:\")\n",
    "    llm_ngrams = ngram_freq[ngram_freq['LLM'] == llm].drop('LLM', axis=1).sum().sort_values(ascending=False).head(5)\n",
    "    print(llm_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d77d6d7-9dec-4890-a948-b88fd4c48b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
